{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwxFg9Nf6eOW0DF68uVuA4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"5sVMqWqUmLAL","executionInfo":{"status":"ok","timestamp":1748497940867,"user_tz":-330,"elapsed":7,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":["class GridWorld:\n","    def __init__(self, N, obstacles, goal):\n","        self.N = N\n","        self.obstacles = obstacles\n","        self.goal = goal\n","        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n","        self.gamma = 0.9\n","        self.theta = 1e-4\n","        self.reward_default = -1\n","        self.reward_goal = 100\n","        self.reward_obstacle = -5\n","        self.policy = np.zeros((N, N, 4)) + 0.25\n","        self.value_function = np.zeros((N, N))\n","\n","    def is_valid(self, x, y):\n","        return 0 <= x < self.N and 0 <= y < self.N and (x, y) not in self.obstacles\n","\n","    def value_iteration(self):\n","        while True:\n","            delta = 0\n","            new_value_function = np.copy(self.value_function)\n","            for i in range(self.N):\n","                for j in range(self.N):\n","                    if (i, j) == self.goal:\n","                        continue\n","                    v = []\n","                    for a, (dx, dy) in enumerate(self.actions):\n","                        nx, ny = i + dx, j + dy\n","                        if not self.is_valid(nx, ny):\n","                            reward = self.reward_obstacle\n","                            nx, ny = i, j\n","                        else:\n","                            reward = self.reward_goal if (nx, ny) == self.goal else self.reward_default\n","                        v.append(reward + self.gamma * self.value_function[nx, ny])\n","                    new_value_function[i, j] = max(v)\n","                    delta = max(delta, abs(self.value_function[i, j] - new_value_function[i, j]))\n","            self.value_function = new_value_function\n","            if delta < self.theta:\n","                break\n","\n","    def policy_iteration(self):\n","        policy_stable = False\n","        while not policy_stable:\n","            self.policy_evaluation()\n","            policy_stable = self.policy_improvement()\n","\n","    def policy_evaluation(self):\n","        while True:\n","            delta = 0\n","            new_value_function = np.copy(self.value_function)\n","            for i in range(self.N):\n","                for j in range(self.N):\n","                    if (i, j) == self.goal:\n","                        continue\n","                    v = 0\n","                    for a, (dx, dy) in enumerate(self.actions):\n","                        nx, ny = i + dx, j + dy\n","                        if not self.is_valid(nx, ny):\n","                            reward = self.reward_obstacle\n","                            nx, ny = i, j\n","                        else:\n","                            reward = self.reward_goal if (nx, ny) == self.goal else self.reward_default\n","                        v += self.policy[i, j, a] * (reward + self.gamma * self.value_function[nx, ny])\n","                    new_value_function[i, j] = v\n","                    delta = max(delta, abs(self.value_function[i, j] - new_value_function[i, j]))\n","            self.value_function = new_value_function\n","            if delta < self.theta:\n","                break\n","\n","    def policy_improvement(self):\n","        policy_stable = True\n","        new_policy = np.zeros_like(self.policy)\n","        for i in range(self.N):\n","            for j in range(self.N):\n","                if (i, j) == self.goal:\n","                    continue\n","                values = []\n","                for a, (dx, dy) in enumerate(self.actions):\n","                    nx, ny = i + dx, j + dy\n","                    if not self.is_valid(nx, ny):\n","                        reward = self.reward_obstacle\n","                        nx, ny = i, j\n","                    else:\n","                        reward = self.reward_goal if (nx, ny) == self.goal else self.reward_default\n","                    values.append(reward + self.gamma * self.value_function[nx, ny])\n","                best_action = np.argmax(values)\n","                new_policy[i, j, :] = 0\n","                new_policy[i, j, best_action] = 1\n","                if not np.array_equal(self.policy[i, j], new_policy[i, j]):\n","                    policy_stable = False\n","        self.policy = new_policy\n","        return policy_stable"],"metadata":{"id":"cHcHaH5hmUAR","executionInfo":{"status":"ok","timestamp":1748497993944,"user_tz":-330,"elapsed":31,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["N = 5\n","obstacles = {(1, 1), (2, 2), (3, 3)}\n","goal = (4, 4)\n","gw = GridWorld(N, obstacles, goal)\n","print(\"Performing Value Iteration...\")\n","gw.value_iteration()\n","print(\"Optimal Value Function:\")\n","print(gw.value_function)\n","print(\"Performing Policy Iteration...\")\n","gw.policy_iteration()\n","print(\"Optimal Policy (Probability of Actions at Each State):\")\n","print(gw.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZ3r21JrmX1V","executionInfo":{"status":"ok","timestamp":1748498000484,"user_tz":-330,"elapsed":76,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}},"outputId":"209f0d50-ed0a-4a81-b564-92a2c1a402b9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Performing Value Iteration...\n","Optimal Value Function:\n","[[ 42.612659  48.45851   54.9539    62.171     70.19    ]\n"," [ 48.45851   54.9539    62.171     70.19      79.1     ]\n"," [ 54.9539    62.171     70.19      79.1       89.      ]\n"," [ 62.171     70.19      79.1       89.       100.      ]\n"," [ 70.19      79.1       89.       100.         0.      ]]\n","Performing Policy Iteration...\n","Optimal Policy (Probability of Actions at Each State):\n","[[[0. 1. 0. 0.]\n","  [0. 0. 0. 1.]\n","  [0. 1. 0. 0.]\n","  [0. 1. 0. 0.]\n","  [0. 1. 0. 0.]]\n","\n"," [[0. 1. 0. 0.]\n","  [0. 1. 0. 0.]\n","  [0. 0. 0. 1.]\n","  [0. 1. 0. 0.]\n","  [0. 1. 0. 0.]]\n","\n"," [[0. 1. 0. 0.]\n","  [0. 1. 0. 0.]\n","  [0. 1. 0. 0.]\n","  [0. 0. 0. 1.]\n","  [0. 1. 0. 0.]]\n","\n"," [[0. 1. 0. 0.]\n","  [0. 1. 0. 0.]\n","  [0. 1. 0. 0.]\n","  [0. 1. 0. 0.]\n","  [0. 1. 0. 0.]]\n","\n"," [[0. 0. 0. 1.]\n","  [0. 0. 0. 1.]\n","  [0. 0. 0. 1.]\n","  [0. 0. 0. 1.]\n","  [0. 0. 0. 0.]]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qqKMC9Kmmiiw"},"execution_count":null,"outputs":[]}]}