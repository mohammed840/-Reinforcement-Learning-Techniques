{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNeuYn008TevQviPdaKBleF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"PsuLu_aOnWir","executionInfo":{"status":"ok","timestamp":1748498234131,"user_tz":-330,"elapsed":674,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"outputs":[],"source":["import gym\n","import numpy as np"]},{"cell_type":"code","source":["def value_iteration(env, gamma=0.9, theta=1e-6):\n","    V = np.zeros(env.observation_space.n)\n","    while True:\n","        delta = 0\n","        for s in range(env.observation_space.n):\n","            Q_values = [sum(prob * (r + gamma * V[s_]) for prob, s_, r, _ in env.P[s][a])\n","                        for a in range(env.action_space.n)]\n","            max_q = max(Q_values)\n","            delta = max(delta, abs(V[s] - max_q))\n","            V[s] = max_q\n","        if delta < theta:\n","            break\n","    policy = np.zeros(env.observation_space.n, dtype=int)\n","    for s in range(env.observation_space.n):\n","        policy[s] = np.argmax([sum(prob * (r + gamma * V[s_]) for prob, s_, r, _ in env.P[s][a])\n","                               for a in range(env.action_space.n)])\n","    return policy, V"],"metadata":{"id":"wBYhAnr2nbbx","executionInfo":{"status":"ok","timestamp":1748498242191,"user_tz":-330,"elapsed":12,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def policy_iteration(env, gamma=0.9, theta=1e-6):\n","    policy = np.random.choice(env.action_space.n, env.observation_space.n)\n","    V = np.zeros(env.observation_space.n)\n","    while True:\n","        while True:\n","            delta = 0\n","            for s in range(env.observation_space.n):\n","                v = V[s]\n","                V[s] = sum(prob * (r + gamma * V[s_]) for prob, s_, r, _ in env.P[s][policy[s]])\n","                delta = max(delta, abs(v - V[s]))\n","            if delta < theta:\n","                break\n","        policy_stable = True\n","        for s in range(env.observation_space.n):\n","            old_action = policy[s]\n","            policy[s] = np.argmax([sum(prob * (r + gamma * V[s_]) for prob, s_, r, _ in env.P[s][a])\n","                                   for a in range(env.action_space.n)])\n","            if old_action != policy[s]:\n","                policy_stable = False\n","        if policy_stable:\n","            break\n","    return policy, V"],"metadata":{"id":"Ix5RWl5Pndj3","executionInfo":{"status":"ok","timestamp":1748498253289,"user_tz":-330,"elapsed":88,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def run_taxi_rl():\n","    env = gym.make(\"Taxi-v3\")\n","    print(\"Running Value Iteration...\")\n","    policy_vi, V_vi = value_iteration(env)\n","    print(\"Optimal Policy (Value Iteration):\", policy_vi)\n","    print(\"Running Policy Iteration...\")\n","    policy_pi, V_pi = policy_iteration(env)\n","    print(\"Optimal Policy (Policy Iteration):\", policy_pi)\n","    env.close()"],"metadata":{"id":"qtS71q3ongQj","executionInfo":{"status":"ok","timestamp":1748498261001,"user_tz":-330,"elapsed":13,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    run_taxi_rl()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASKc48X6niKW","executionInfo":{"status":"ok","timestamp":1748498270143,"user_tz":-330,"elapsed":3980,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}},"outputId":"fe29c496-5e40-4bd4-84c2-deb4b4d0e41f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["Running Value Iteration...\n","Optimal Policy (Value Iteration): [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n"," 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n"," 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 2 2 2\n"," 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1\n"," 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n"," 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 2 2 2 2 3 3 3 3 2 2 2 2 3 2 3\n"," 2 3 3 3 3 2 2 2 2 3 3 3 3 0 0 0 0 3 2 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n"," 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 2 2 2 2 1 1 1 1 2\n"," 2 2 2 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1\n"," 1 1 0 0 0 0 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n"," 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 2 2 2 2 1 1 1 1 2 2 2 2 1 2 1 2 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 1 4 4 4 4 1 2 1 5 1\n"," 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3]\n","Running Policy Iteration...\n","Optimal Policy (Policy Iteration): [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n"," 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n"," 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 1 1 1\n"," 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n"," 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n"," 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 2 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n"," 2 3 3 3 3 2 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n"," 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 2 1 1 1 1 1 1 1 2\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n"," 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n"," 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"oKU6-bb7njbB"},"execution_count":null,"outputs":[]}]}