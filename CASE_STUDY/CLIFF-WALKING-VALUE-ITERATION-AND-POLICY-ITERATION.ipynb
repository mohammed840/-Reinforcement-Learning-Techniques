{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKizg890ukpIpEVx0xpHg1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"txxcVWcHoP2Y","executionInfo":{"status":"ok","timestamp":1748498486046,"user_tz":-330,"elapsed":717,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"outputs":[],"source":["import gym\n","import numpy as np"]},{"cell_type":"code","source":["def value_iteration(env, gamma=0.99, theta=1e-6):\n","    V = np.zeros(env.nS)\n","    while True:\n","        delta = 0\n","        for s in range(env.nS):\n","            v = V[s]\n","            q_values = [\n","                sum([prob * (r + gamma * V[s_]) for prob, s_, r, _ in env.P[s][a]])\n","                for a in range(env.nA)\n","            ]\n","            V[s] = max(q_values)\n","            delta = max(delta, abs(v - V[s]))\n","        if delta < theta:\n","            break\n","\n","    policy = np.zeros([env.nS, env.nA])\n","    for s in range(env.nS):\n","        q_values = [\n","            sum([prob * (r + gamma * V[s_]) for prob, s_, r, _ in env.P[s][a]])\n","            for a in range(env.nA)\n","        ]\n","        best_action = np.argmax(q_values)\n","        policy[s, best_action] = 1\n","    return policy, V"],"metadata":{"id":"9V_2-W49oXbr","executionInfo":{"status":"ok","timestamp":1748498488857,"user_tz":-330,"elapsed":20,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def policy_iteration(env, gamma=0.99, theta=1e-6):\n","    policy = np.ones([env.nS, env.nA]) / env.nA\n","    V = np.zeros(env.nS)\n","    while True:\n","        while True:\n","            delta = 0\n","            for s in range(env.nS):\n","                v = V[s]\n","                V[s] = sum([\n","                    policy[s, a] * sum([prob * (r + gamma * V[s_]) for prob, s_, r, _ in env.P[s][a]])\n","                    for a in range(env.nA)\n","                ])\n","                delta = max(delta, abs(v - V[s]))\n","            if delta < theta:\n","                break\n","\n","        stable = True\n","        for s in range(env.nS):\n","            old_action = np.argmax(policy[s])\n","            q_values = [\n","                sum([prob * (r + gamma * V[s_]) for prob, s_, r, _ in env.P[s][a]])\n","                for a in range(env.nA)\n","            ]\n","            best_action = np.argmax(q_values)\n","            policy[s] = np.eye(env.nA)[best_action]\n","            if old_action != best_action:\n","                stable = False\n","        if stable:\n","            break\n","    return policy, V"],"metadata":{"id":"EkczKv6CoZzA","executionInfo":{"status":"ok","timestamp":1748498512963,"user_tz":-330,"elapsed":30,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"CliffWalking-v0\")\n","\n","policy_vi, V_vi = value_iteration(env)\n","policy_pi, V_pi = policy_iteration(env)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onINQrjwoc2b","executionInfo":{"status":"ok","timestamp":1748498526833,"user_tz":-330,"elapsed":7205,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}},"outputId":"7bc0a580-6441-4670-80d2-5ea3026105d0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["print(\"Optimal Policy from Value Iteration:\")\n","print(policy_vi.reshape((4, 12, 4)))  # 4 rows x 12 columns x 4 actions\n","\n","print(\"\\nOptimal Policy from Policy Iteration:\")\n","print(policy_pi.reshape((4, 12, 4)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaxU3E59ohUQ","executionInfo":{"status":"ok","timestamp":1748498527528,"user_tz":-330,"elapsed":41,"user":{"displayName":"Kishore M","userId":"10139779576246080488"}},"outputId":"7e9a0ecb-a7aa-43d4-f6c9-05e8277af2ba"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy from Value Iteration:\n","[[[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]]\n","\n","Optimal Policy from Policy Iteration:\n","[[[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"sxj4pR7ZojPI"},"execution_count":null,"outputs":[]}]}